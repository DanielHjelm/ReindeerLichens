{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "769738ed",
   "metadata": {},
   "source": [
    "# Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29c6e8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, MaxPooling2D, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.layers import LayerNormalization, MultiHeadAttention, Input, LSTM, Attention, AveragePooling2D, Concatenate, GlobalAveragePooling2D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b23025c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model architecture uses a combination of convolutional and LSTM layers to extract spatial and temporal features from the input data, \n",
    "# and an attention layer to focus on the most relevant features for the task. \n",
    "# This type of architecture has been shown to perform well on a variety of tasks, including image and video classification, natural language processing, and speech recognition.\n",
    "\n",
    "def LSTM_Attention(input_shape, num_classes):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv2D(64, (3, 3), padding='same', activation='relu')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    x = Conv2D(256, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(256, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    x = LSTM(256, return_sequences=True)(x)\n",
    "    x = Attention()(x)\n",
    "\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f48eb1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this article: https://reader.elsevier.com/reader/sd/pii/S2468785522002312?token=DF0636838957FD7127260415FABFB0B0485A88DDB54E84357BDE1D1B2E7F7D0D9374FEA27ED601525C4B8FAEEF7BDCC3&originRegion=eu-west-1&originCreation=20221208094015\n",
    "# They use Google Inception V3: https://keras.io/api/applications/inceptionv3/\n",
    "\n",
    "# Define the inception module\n",
    "def inception_module(x, filters):\n",
    "    conv1 = Conv2D(filters[0], (1, 1), padding='same', activation='relu')(x)\n",
    "\n",
    "    conv3 = Conv2D(filters[1], (1, 1), padding='same', activation='relu')(x)\n",
    "    conv3 = Conv2D(filters[2], (3, 3), padding='same', activation='relu')(conv3)\n",
    "\n",
    "    conv5 = Conv2D(filters[3], (1, 1), padding='same', activation='relu')(x)\n",
    "    conv5 = Conv2D(filters[4], (5, 5), padding='same', activation='relu')(conv5)\n",
    "\n",
    "    pool = MaxPooling2D((3, 3), strides=(1, 1), padding='same')(x)\n",
    "    pool = Conv2D(filters[5], (1, 1), padding='same', activation='relu')(pool)\n",
    "\n",
    "    x = Concatenate()([conv1, conv3, conv5, pool])\n",
    "\n",
    "    return x\n",
    "\n",
    "# Define the model architecture\n",
    "def create_model(input_shape, num_classes):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv2D(32, (3, 3), padding='same', strides=(2, 2), activation='relu')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(32, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "    x = Conv2D(80, (1, 1), padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(192, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "\n",
    "    x = inception_module(x, 64, (96, 128), (16, 32), 32)\n",
    "    x = inception_module(x, 128, (128, 192), (32, 96), 64)\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "\n",
    "    x = inception_module(x, 192, (96, 208), (16, 48), 64)\n",
    "    x = inception_module(x, 160, (112, 224), (24, 64), 64)\n",
    "    x = inception_module(x, 128, (128, 256), (24, 64), 64)\n",
    "    x = inception_module(x, 112, (144, 288), (32, 64), 64)\n",
    "    x = inception_module(x, 256, (160, 320), (32, 128), 128)\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "\n",
    "    x = inception_module(x, 256, (160, 320), (32, 128), 128)\n",
    "    x = inception_module(x, 384, (192, 384), (48, 128), 128)\n",
    "\n",
    "    x = AveragePooling2D((2, 2), strides=(1, 1), padding='same')(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e004ebe7",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "https://towardsdatascience.com/metrics-to-evaluate-your-semantic-segmentation-model-6bcb99639aa2\n",
    "\n",
    "## Intersection-over-union \n",
    "The intersection-over-union (IoU) metric measures the overlap between the predicted and ground-truth segmentation masks. It is calculated by dividing the intersection of the two masks by their union, and is commonly used to evaluate the performance of binary and multi-class segmentation models.\n",
    "\n",
    "## Pixel accuracy\n",
    "The pixel accuracy metric measures the percentage of pixels that are correctly classified by the model. It is calculated by dividing the number of correctly classified pixels by the total number of pixels in the image, and is commonly used to evaluate the performance of multi-class segmentation models.\n",
    "\n",
    "## Mean Average Precision (mAP)\n",
    "The mean average precision (mAP) metric measures the average precision of the model across all classes. It is calculated by first computing the average precision for each class, and then averaging the results across all classes. The mAP metric is commonly used to evaluate the performance of object detection and segmentation models.\n",
    "\n",
    "## Dice coefficient\n",
    "Dice coefficient is a measure of the overlap between two sets of data. It is often used in the field of image analysis to compare the similarity of two images, but it can be applied to any kind of data where it is meaningful to compare the overlap between two sets. The Dice coefficient is calculated as the ratio of the number of elements that are present in both sets to the total number of elements in both sets. A Dice coefficient of 1 indicates that the two sets are identical, while a coefficient of 0 indicates that there is no overlap between the sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c69d521f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the intersection-over-union (IoU)\n",
    "def iou(y_true, y_pred):\n",
    "    # Flatten the predicted and ground-truth masks\n",
    "    y_true_flat = tf.reshape(y_true, [-1])\n",
    "    y_pred_flat = tf.reshape(y_pred, [-1])\n",
    "\n",
    "    # Compute the intersection and union\n",
    "    intersection = tf.reduce_sum(y_true_flat * y_pred_flat)\n",
    "    union = tf.reduce_sum(y_true_flat) + tf.reduce_sum(y_pred_flat) - intersection\n",
    "\n",
    "    # Compute the IoU\n",
    "    iou = intersection / union\n",
    "\n",
    "    return iou\n",
    "\n",
    "# Calculate the pixel accuracy\n",
    "def pixel_accuracy(y_true, y_pred):\n",
    "    # Flatten the predicted and ground-truth masks\n",
    "    y_true_flat = tf.reshape(y_true, [-1])\n",
    "    y_pred_flat = tf.reshape(y_pred, [-1])\n",
    "\n",
    "    # Compute the pixel accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(y_true_flat, y_pred_flat), tf.float32))\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Calculate the mean average precision (mAP)\n",
    "def mean_average_precision(y_true, y_pred):\n",
    "    # Flatten the predicted and ground-truth masks\n",
    "    y_true_flat = tf.reshape(y_true, [-1, num_classes])\n",
    "    y_pred_flat = tf.reshape(y_pred, [-1, num_classes])\n",
    "\n",
    "    # Compute the average precision for each class\n",
    "    precisions = []\n",
    "    for i in range(num_classes):\n",
    "        precision, _ = tf.metrics.precision_at_k(y_true_flat[:, i], y_pred_flat[:, i], 1)\n",
    "        precisions.append(precision)\n",
    "\n",
    "    # Compute the mAP\n",
    "    mAP = tf.reduce_mean(precisions)\n",
    "\n",
    "    return mAP\n",
    "\n",
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "  intersection = K.sum(y_true * y_pred, axis=[1,2,3])\n",
    "  union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3])\n",
    "  dice = K.mean((2. * intersection + smooth)/(union + smooth), axis=0)\n",
    "  return dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe7646b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/daniel/Desktop/ImagestoDBWithoutFrame'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hej = \"/Users/daniel/Desktop/ImagestoDBWithoutFrame/IMG_0182.JPG\"\n",
    "\n",
    "hej.rpartition('/')[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc585a56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'IMG_0182_mask.JPG'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split[0]+\"_mask.\"+split[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d797ad55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8 (default, Apr 13 2021, 12:59:45) \n[Clang 10.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "5637242735df105300a10f363d5c0d18b4b77d7105ef62addc8c3980e22cd89c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
